{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.10.12' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/usr/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import logging\n",
    "from tqdm import tqdm\n",
    "from matplotlib import pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "logger = logging.getLogger(__name__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.10.12' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/usr/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "!/usr/bin/python3 -m pip install ipykernel -U --user --force-reinstall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "datapath = \"/cluster/work/igp_psr/arrueegg/GNSS_STEC_DB/2024/322/ccl_2024322_30_5.h5\"\n",
    "# datapath = \"V:/courses/dslab/team16/data/2023/020/ccl_2023020_30_5.h5\"\n",
    "\n",
    "def get_data(datapath: str) -> pd.DataFrame:\n",
    "    file = h5py.File(datapath, 'r')\n",
    "    year = list(file.keys())[0]\n",
    "    day = list(file[year].keys())[0]\n",
    "    data = file[year][day][\"all_data\"]\n",
    "    # TODO: figure out a better way than pandas to manipulate the data.\n",
    "    data = pd.DataFrame(data[:])\n",
    "    data[\"station\"] = data[\"station\"].str.decode(\"utf8\")\n",
    "    data[\"sat\"] = data[\"sat\"].str.decode(\"utf8\")\n",
    "\n",
    "    logger.info(\"Loaded data from file.\")\n",
    "    return data\n",
    "\n",
    "\n",
    "def split_train_val_test(data: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\" If file with splits alrady exists, load it. Otherwise generate the splits.\n",
    "    I do it this way to ensure that the split is reproducible. Other options considered:\n",
    "    - Sort the stations and randomly shuffle them using a fixed seed. After shuffling, keep first section\n",
    "      for training, second for validation and third for testing. This would fail if depending on the day\n",
    "      some stations are inactive.\n",
    "    - Hash the stations by name into train, val and test buckets. My concern is that the hashing function\n",
    "      may not be reproducible if, for instance, OS changes.\n",
    "    \"\"\"\n",
    "\n",
    "    split_file_path = \"/cluster/work/igp_psr/dslab_FS25_data_and_weights/split.json\"\n",
    "    if os.path.exists(split_file_path):\n",
    "        with open(split_file_path, 'r') as f:\n",
    "            stations_map = json.load(f)\n",
    "        logger.info(\"Loading train, val, test split...\")\n",
    "        \n",
    "        \n",
    "    else:\n",
    "        logger.info(\"No existing split found...\")\n",
    "        logger.info(\"Splitting data into train, val, and test sets...\")\n",
    "        stations_df = data[[\"station\", \"lat_sta\", \"lon_sta\"]].groupby(\"station\").agg(\"first\").sort_values(by=\"station\").reset_index()\n",
    "        stations = stations_df[\"station\"].tolist()\n",
    "        np.random.seed(10)  \n",
    "        np.random.shuffle(stations)\n",
    "        nstations = len(stations)\n",
    "        stations_map = {station: (i / nstations > 0.6) + (i / nstations > 0.8) for i, station in enumerate(stations)}\n",
    "    \n",
    "        fig, ax = plt.subplots()\n",
    "        stations_df[\"split\"] = stations_df[\"station\"].map(stations_map)\n",
    "           \n",
    "        ax.scatter(stations_df[\"lon_sta\"], stations_df[\"lat_sta\"], c=stations_df[\"split\"] / 2, cmap=\"viridis\")\n",
    "        fig.savefig(\"outputs/split.png\")\n",
    "        \n",
    "        with open(split_file_path, 'w') as f:\n",
    "            json.dump(stations_map, f)\n",
    "    \n",
    "    \n",
    "    data['split'] = data[\"station\"].map(stations_map)   \n",
    "    logger.info(\"Train, val, and test sets ready.\") \n",
    "    \n",
    "    return data\n",
    "\n",
    "def extract_features(data: pd.DataFrame) -> pd.DataFrame:\n",
    "    logger.info(\"Extracting sin/cos features...\")\n",
    "    data['sm_lon_ipp_s'] = np.sin(2 * np.pi * data['sm_lon_ipp'] / 360)\n",
    "    data['sm_lon_ipp_c'] = np.cos(2 * np.pi * data['sm_lon_ipp'] / 360)\n",
    "    data['sod_s'] = np.sin(2 * np.pi * data['sod'] / 86400) \n",
    "    data['sod_c'] = np.cos(2 * np.pi * data['sod'] / 86400) \n",
    "    data['satazi_s'] = np.sin(2 * np.pi * data['satazi'] / 360)\n",
    "    data['satazi_c'] = np.cos(2 * np.pi * data['satazi'] / 360)\n",
    "\n",
    "    logger.info(\"Extracted sin/cos features.\")\n",
    "    return data\n",
    "    \n",
    "class FCN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(8, 256)\n",
    "        self.fc2 = nn.Linear(256, 256)\n",
    "        self.fc3 = nn.Linear(256, 1)\n",
    "        self.double()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "    \n",
    "def train(dataloader, model, loss_fct, optimizer, device):\n",
    "    size = len(dataloader.dataset)\n",
    "    # Set the model to training mode - important for batch normalization and dropout layers\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for batch, (X, y) in tqdm(enumerate(dataloader), total=len(dataloader)):\n",
    "        # Compute prediction and loss\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        pred = model(X)\n",
    "        loss = loss_fct(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if batch % 6000 == 5999:    \n",
    "            logger.info(f'[{batch + 1:5d}/{len(dataloader):>5d}] loss: {running_loss / 2000:.3f}')\n",
    "            running_loss = 0.0\n",
    "\n",
    "\n",
    "def test(dataloader, model, loss_fct, device):\n",
    "    # Set the model to evaluation mode - important for batch normalization and dropout layers\n",
    "    model.eval()\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss = 0\n",
    "\n",
    "    # Evaluating the model with torch.no_grad() ensures that no gradients are computed during test mode\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fct(pred, y).item()\n",
    "\n",
    "    test_loss /= num_batches\n",
    "\n",
    "    return test_loss\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    if not os.path.exists(\"outputs\"): \n",
    "        os.makedirs(\"outputs\")\n",
    "                    \n",
    "    torch.manual_seed(10)\n",
    "    logging.basicConfig(filename='outputs/FCN.log', level=logging.INFO, format='%(asctime)s | %(message)s', datefmt='%H:%M')\n",
    "    logger.info(\"-------------------------------------------------------\\nStarting Script\\n-------------------------------------------------------\")\n",
    "\n",
    "    device = torch.accelerator.current_accelerator().type if torch.cuda.is_available() else \"cpu\"\n",
    "    logger.info(\"Using %s device\", device)\n",
    "\n",
    "    learning_rate = 1e-3\n",
    "    batch_size = 64\n",
    "    epochs = 10\n",
    "\n",
    "    logger.info(\"Loading Data...\")\n",
    "    data = get_data(datapath)\n",
    "    data = split_train_val_test(data)\n",
    "    data = extract_features(data)\n",
    "    features = ['sm_lat_ipp', 'sm_lon_ipp_s', 'sm_lon_ipp_c', 'sod_s', 'sod_c', 'satele','satazi_s', 'satazi_c']\n",
    "\n",
    "    logger.info(\"Loading Data into tensors...\")\n",
    "    X = torch.tensor(data[features].values, device=device, dtype=torch.float64)\n",
    "    y = torch.tensor(data['stec'].values, device=device, dtype=torch.float64).unsqueeze_(-1)\n",
    "\n",
    "    logger.info(\"Creating Tensor Datasets...\")\n",
    "    dataset_train = TensorDataset(X[data['split'] == 0], y[data['split'] == 0])\n",
    "    dataset_val = TensorDataset(X[data['split'] == 1], y[data['split'] == 1])\n",
    "    dataset_test = TensorDataset(X[data['split'] == 2], y[data['split'] == 2])\n",
    "    \n",
    "    logger.info(\"Preparing DataLoaders...\")\n",
    "    dataloader_train = DataLoader(dataset_train, batch_size=batch_size, shuffle=True)\n",
    "    dataloader_val = DataLoader(dataset_val, batch_size=batch_size, shuffle=True)\n",
    "    dataloader_test = DataLoader(dataset_test, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "    logger.info(\"Setting up Model...\")\n",
    "    model = FCN().to(device)\n",
    "    logger.info(\"Model: %s\", model)\n",
    "\n",
    "    loss = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    logger.info(\"Starting training...\")\n",
    "    best_val_loss = float('inf')\n",
    "    for t in range(epochs):\n",
    "        logger.info(\"-------------------------------\\nEpoch %s\\n-------------------------------\", t+1)\n",
    "        train(dataloader_train, model, loss, optimizer, device)\n",
    "        val_loss = test(dataloader_val, model, loss, device)\n",
    "        logger.info(f\"Validation Loss: {val_loss:>7f}\")\n",
    "        \n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), \"model.pth\")\n",
    "        else:\n",
    "            # validation loss is increasing, so we stop training\n",
    "            logger.info(\"Validation loss increased. Stopping training.\")\n",
    "            break\n",
    "        \n",
    "    logger.info(\"Training completed.\")\n",
    "\n",
    "\n",
    "    logger.info(\"Starting evaluation...\")\n",
    "    eval_loss_fct = nn.L1Loss()\n",
    "    test_loss = test(dataloader_test, model, eval_loss_fct, device)\n",
    "    logger.info(f\"Evaluation MAE Loss: {test_loss:>7f}\")\n",
    "    logger.info(\"Completed.\")\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
